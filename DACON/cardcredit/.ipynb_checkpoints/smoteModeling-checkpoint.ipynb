{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 증강 \n",
    "\n",
    "credit 변수의 분포가 균일하지 않음.    \n",
    "2 : 16968, 1 : 6267, 0 : 3222\n",
    "\n",
    "\n",
    "데이터의 특성 상 imbalanced 할 수 밖에 없음.    \n",
    "그러나 이러한 데이터셋을 그대로 사용할 경우, 많은 수를 가진 레이블을 지정할 가능성이 높음. 이를 해결하기 위해 증강 기법을 사용해보고자 함.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('card_dataset/train.csv')\n",
    "test = pd.read_csv('card_dataset/test.csv')\n",
    "submission =pd.read_csv('card_dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = train.copy()\n",
    "new_test = test.copy()\n",
    "\n",
    "new_train[['work_phone','phone','email','FLAG_MOBIL']] = new_train[['work_phone','phone','email','FLAG_MOBIL']].astype('object')\n",
    "new_test[['work_phone','phone','email','FLAG_MOBIL']] = new_test[['work_phone','phone','email','FLAG_MOBIL']].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. income_type = 'Pensioner' 일때 occyp_type = 'Retired' and DAYS_EMPLOYED \n",
    "new_train.loc[new_train['income_type'] == 'Pensioner', 'occyp_type'] = 'Retired'\n",
    "new_train.loc[new_train['income_type'] == 'Pensioner', 'DAYS_EMPLOYED'] = 0 \n",
    "\n",
    "## 2. occyp_type = NaN & income_type = 'State servant'인 경우 occyp_type ='State servant' 로 변경\n",
    "new_train.loc[(new_train.occyp_type.isna())  & (new_train.income_type == 'State servant'), 'occyp_type'] = 'State servant'\n",
    "\n",
    "## 3. occyp_type = NaN & income_type  in ('working', 'Commercial associate', 'Student) 일 경우 occyp_type ='Extra staff'로 변경 \n",
    "new_train.loc[(new_train.occyp_type.isna())  & (new_train.income_type.isin(['Working','Commercial associate','Student'])), 'occyp_type'] = 'Extra staff'\n",
    "\n",
    "\n",
    "## 1. income_type = 'Pensioner' 일때 occyp_type = 'Retired' and DAYS_EMPLOYED \n",
    "new_test.loc[new_test['income_type'] == 'Pensioner', 'occyp_type'] = 'Retired'\n",
    "new_test.loc[new_test['income_type'] == 'Pensioner', 'DAYS_EMPLOYED'] = 0 \n",
    "\n",
    "## 2. occyp_type = NaN & income_type = 'State servant'인 경우 occyp_type ='State servant' 로 변경\n",
    "new_test.loc[(new_test.occyp_type.isna())  & (new_test.income_type == 'State servant'), 'occyp_type'] = 'State servant' \n",
    "\n",
    "## 3. occyp_type = NaN & income_type  in ('working', 'Commercial associate', 'Student) 일 경우 occyp_type ='Extra staff'로 변경 \n",
    "new_test.loc[(new_test.occyp_type.isna())  & (new_test.income_type.isin(['Working','Commercial associate','Student'])), 'occyp_type'] = 'Extra staff' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. FLAG_MOBIL 변수 삭제\n",
    "m_train = new_train.drop('FLAG_MOBIL', axis = 1)\n",
    "m_test = new_test.drop(['FLAG_MOBIL','index'], axis =1)\n",
    "\n",
    "\n",
    "# 2. credit 변수 분할 \n",
    "target_credit = m_train['credit']\n",
    "target_credit = target_credit.astype('int')\n",
    "\n",
    "m_train_x = m_train.drop(['index','credit'], axis = 1)\n",
    "\n",
    "\n",
    "# 3. 더미변수 \n",
    "dum_train_x = pd.get_dummies(m_train_x, drop_first = True)\n",
    "dum_test_x = pd.get_dummies(m_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_credit=target_credit.astype('int')\n",
    "train_x, test_x, train_y, test_y = train_test_split(dum_train_x, target_credit, \n",
    "                                                    random_state = 507,\n",
    "                                                    test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, \n",
    "                        shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.2 µs\n",
      "====================================1============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.706519\tvalid_1's multi_logloss: 0.770599\n",
      "[2000]\ttraining's multi_logloss: 0.640601\tvalid_1's multi_logloss: 0.750577\n",
      "[3000]\ttraining's multi_logloss: 0.590134\tvalid_1's multi_logloss: 0.740195\n",
      "[4000]\ttraining's multi_logloss: 0.549255\tvalid_1's multi_logloss: 0.734773\n",
      "[5000]\ttraining's multi_logloss: 0.512804\tvalid_1's multi_logloss: 0.730157\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's multi_logloss: 0.512804\tvalid_1's multi_logloss: 0.730157\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================2============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.701369\tvalid_1's multi_logloss: 0.780179\n",
      "[2000]\ttraining's multi_logloss: 0.635666\tvalid_1's multi_logloss: 0.762242\n",
      "[3000]\ttraining's multi_logloss: 0.585638\tvalid_1's multi_logloss: 0.754024\n",
      "Early stopping, best iteration is:\n",
      "[3451]\ttraining's multi_logloss: 0.56597\tvalid_1's multi_logloss: 0.751443\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================3============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.703462\tvalid_1's multi_logloss: 0.777937\n",
      "[2000]\ttraining's multi_logloss: 0.64041\tvalid_1's multi_logloss: 0.761274\n",
      "[3000]\ttraining's multi_logloss: 0.587323\tvalid_1's multi_logloss: 0.751833\n",
      "[4000]\ttraining's multi_logloss: 0.544392\tvalid_1's multi_logloss: 0.746478\n",
      "Early stopping, best iteration is:\n",
      "[4034]\ttraining's multi_logloss: 0.543102\tvalid_1's multi_logloss: 0.746374\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================4============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.705976\tvalid_1's multi_logloss: 0.775028\n",
      "[2000]\ttraining's multi_logloss: 0.638559\tvalid_1's multi_logloss: 0.754582\n",
      "[3000]\ttraining's multi_logloss: 0.589532\tvalid_1's multi_logloss: 0.745903\n",
      "Early stopping, best iteration is:\n",
      "[3948]\ttraining's multi_logloss: 0.551294\tvalid_1's multi_logloss: 0.741883\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================5============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.70449\tvalid_1's multi_logloss: 0.77129\n",
      "[2000]\ttraining's multi_logloss: 0.638401\tvalid_1's multi_logloss: 0.753526\n",
      "[3000]\ttraining's multi_logloss: 0.588137\tvalid_1's multi_logloss: 0.743889\n",
      "[4000]\ttraining's multi_logloss: 0.547595\tvalid_1's multi_logloss: 0.737103\n",
      "Early stopping, best iteration is:\n",
      "[4213]\ttraining's multi_logloss: 0.539706\tvalid_1's multi_logloss: 0.736592\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "lgb_models={}\n",
    "loss = []\n",
    "sub = np.zeros((dum_test_x.shape[0], 3))\n",
    "\n",
    "for n_folds, (train_index, val_index) in enumerate(folds.split(dum_train_x, target_credit)):\n",
    "    \n",
    "    print(f'===================================={n_folds+1}============================================')\n",
    "\n",
    "    X_train, X_val = dum_train_x.iloc[train_index], dum_train_x.iloc[val_index]\n",
    "    y_train, y_val = target_credit.iloc[train_index], target_credit.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    lgb = LGBMClassifier(n_estimators=5000,\n",
    "                        is_unbalance = True, n_jobs = -1,\n",
    "                        num_leaves = 64,\n",
    "                        max_depth = 10,\n",
    "                        random_state = 507,\n",
    "                        learning_rate = 0.003)\n",
    "    lgb.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "            early_stopping_rounds=30,\n",
    "           verbose=1000)\n",
    "    y_val_onehot = pd.get_dummies(y_val)\n",
    "    prediction = lgb.predict_proba(X_val)\n",
    "    loss.append(log_loss(y_val_onehot, prediction))\n",
    "    sub+= lgb.predict_proba(dum_test_x)\n",
    "    \n",
    "    \n",
    "    print(f'================================================================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7412896726455214"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_over,y_train_over = smote.fit_sample(dum_train_x,target_credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n",
      "====================================1============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's multi_logloss: 0.690662\tvalid_1's multi_logloss: 0.737887\n",
      "[1000]\ttraining's multi_logloss: 0.56486\tvalid_1's multi_logloss: 0.647068\n",
      "[1500]\ttraining's multi_logloss: 0.478613\tvalid_1's multi_logloss: 0.588451\n",
      "[2000]\ttraining's multi_logloss: 0.421223\tvalid_1's multi_logloss: 0.556468\n",
      "[2500]\ttraining's multi_logloss: 0.378916\tvalid_1's multi_logloss: 0.536918\n",
      "[3000]\ttraining's multi_logloss: 0.344483\tvalid_1's multi_logloss: 0.522703\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.344483\tvalid_1's multi_logloss: 0.522703\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================2============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's multi_logloss: 0.690503\tvalid_1's multi_logloss: 0.742245\n",
      "[1000]\ttraining's multi_logloss: 0.561129\tvalid_1's multi_logloss: 0.642458\n",
      "[1500]\ttraining's multi_logloss: 0.48162\tvalid_1's multi_logloss: 0.589006\n",
      "[2000]\ttraining's multi_logloss: 0.42503\tvalid_1's multi_logloss: 0.557808\n",
      "[2500]\ttraining's multi_logloss: 0.381751\tvalid_1's multi_logloss: 0.536441\n",
      "[3000]\ttraining's multi_logloss: 0.346608\tvalid_1's multi_logloss: 0.521204\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.346608\tvalid_1's multi_logloss: 0.521204\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================3============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's multi_logloss: 0.690764\tvalid_1's multi_logloss: 0.735605\n",
      "[1000]\ttraining's multi_logloss: 0.562425\tvalid_1's multi_logloss: 0.639281\n",
      "[1500]\ttraining's multi_logloss: 0.480111\tvalid_1's multi_logloss: 0.583524\n",
      "[2000]\ttraining's multi_logloss: 0.424187\tvalid_1's multi_logloss: 0.553173\n",
      "[2500]\ttraining's multi_logloss: 0.382039\tvalid_1's multi_logloss: 0.534986\n",
      "[3000]\ttraining's multi_logloss: 0.346514\tvalid_1's multi_logloss: 0.520969\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.346514\tvalid_1's multi_logloss: 0.520969\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================4============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's multi_logloss: 0.68773\tvalid_1's multi_logloss: 0.744741\n",
      "[1000]\ttraining's multi_logloss: 0.568389\tvalid_1's multi_logloss: 0.659939\n",
      "[1500]\ttraining's multi_logloss: 0.48186\tvalid_1's multi_logloss: 0.600793\n",
      "[2000]\ttraining's multi_logloss: 0.425431\tvalid_1's multi_logloss: 0.56777\n",
      "[2500]\ttraining's multi_logloss: 0.381365\tvalid_1's multi_logloss: 0.546811\n",
      "[3000]\ttraining's multi_logloss: 0.346796\tvalid_1's multi_logloss: 0.532965\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.346796\tvalid_1's multi_logloss: 0.532965\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================5============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's multi_logloss: 0.686143\tvalid_1's multi_logloss: 0.732743\n",
      "[1000]\ttraining's multi_logloss: 0.564852\tvalid_1's multi_logloss: 0.643217\n",
      "[1500]\ttraining's multi_logloss: 0.485758\tvalid_1's multi_logloss: 0.592\n",
      "[2000]\ttraining's multi_logloss: 0.42894\tvalid_1's multi_logloss: 0.559238\n",
      "[2500]\ttraining's multi_logloss: 0.387009\tvalid_1's multi_logloss: 0.539262\n",
      "[3000]\ttraining's multi_logloss: 0.35117\tvalid_1's multi_logloss: 0.524197\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.35117\tvalid_1's multi_logloss: 0.524197\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "lgb_models={}\n",
    "loss = []\n",
    "sub = np.zeros((dum_test_x.shape[0], 3))\n",
    "\n",
    "for n_folds, (train_index, val_index) in enumerate(folds.split(X_train_over, y_train_over)):\n",
    "    \n",
    "    print(f'===================================={n_folds+1}============================================')\n",
    "\n",
    "    X_train, X_val = X_train_over.iloc[train_index], X_train_over.iloc[val_index]\n",
    "    y_train, y_val = y_train_over.iloc[train_index], y_train_over.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    lgb = LGBMClassifier(n_estimators=3000,\n",
    "                        is_unbalance = True, n_jobs = -1,\n",
    "                        num_leaves = 64,\n",
    "                        max_depth = 14,\n",
    "                        random_state = 507,\n",
    "                        learning_rate = 0.01)\n",
    "    lgb.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "            early_stopping_rounds=30,\n",
    "           verbose=500)\n",
    "    y_val_onehot = pd.get_dummies(y_val)\n",
    "    prediction = lgb.predict_proba(X_val)\n",
    "    loss.append(log_loss(y_val_onehot, prediction))\n",
    "    sub+= lgb.predict_proba(dum_test_x)\n",
    "    \n",
    "    \n",
    "    print(f'================================================================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5244076009775325"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = sub/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/smote.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_list = {'Academic degree':5,\n",
    "           'Higher education':4,\n",
    "           'Incomplete higher':3,\n",
    "           'Secondary / secondary special':2,\n",
    "           'Lower secondary':1}\n",
    "\n",
    "\n",
    "new_train['edu_type'] = train['edu_type'].map(edu_list)\n",
    "new_test['edu_type'] = test['edu_type'].map(edu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. FLAG_MOBIL 변수 삭제\n",
    "m_train = new_train.drop('FLAG_MOBIL', axis = 1)\n",
    "m_test = new_test.drop(['FLAG_MOBIL','index'], axis =1)\n",
    "\n",
    "\n",
    "# 2. credit 변수 분할 \n",
    "target_credit = m_train['credit']\n",
    "target_credit = target_credit.astype('int')\n",
    "\n",
    "m_train_x = m_train.drop(['index','credit'], axis = 1)\n",
    "\n",
    "\n",
    "# 3. 더미변수 \n",
    "dum_train_x = pd.get_dummies(m_train_x, drop_first = True)\n",
    "dum_test_x = pd.get_dummies(m_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=10, \n",
    "                        shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=0)\n",
    "X_train_over,y_train_over = smote.fit_sample(dum_train_x,target_credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================1============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.590267\tvalid_1's multi_logloss: 0.663464\n",
      "[2000]\ttraining's multi_logloss: 0.447592\tvalid_1's multi_logloss: 0.573876\n",
      "[3000]\ttraining's multi_logloss: 0.364263\tvalid_1's multi_logloss: 0.530792\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.364263\tvalid_1's multi_logloss: 0.530792\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================2============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.593209\tvalid_1's multi_logloss: 0.670981\n",
      "[2000]\ttraining's multi_logloss: 0.450928\tvalid_1's multi_logloss: 0.574692\n",
      "[3000]\ttraining's multi_logloss: 0.364676\tvalid_1's multi_logloss: 0.52628\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.364676\tvalid_1's multi_logloss: 0.52628\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================3============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.594447\tvalid_1's multi_logloss: 0.664883\n",
      "[2000]\ttraining's multi_logloss: 0.447583\tvalid_1's multi_logloss: 0.561464\n",
      "[3000]\ttraining's multi_logloss: 0.363594\tvalid_1's multi_logloss: 0.515687\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.363594\tvalid_1's multi_logloss: 0.515687\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================4============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.589749\tvalid_1's multi_logloss: 0.669321\n",
      "[2000]\ttraining's multi_logloss: 0.444462\tvalid_1's multi_logloss: 0.573889\n",
      "[3000]\ttraining's multi_logloss: 0.359903\tvalid_1's multi_logloss: 0.527942\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.359903\tvalid_1's multi_logloss: 0.527942\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================5============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.593909\tvalid_1's multi_logloss: 0.670646\n",
      "[2000]\ttraining's multi_logloss: 0.452131\tvalid_1's multi_logloss: 0.573587\n",
      "[3000]\ttraining's multi_logloss: 0.365846\tvalid_1's multi_logloss: 0.5247\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.365846\tvalid_1's multi_logloss: 0.5247\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================6============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.59045\tvalid_1's multi_logloss: 0.659969\n",
      "[2000]\ttraining's multi_logloss: 0.452724\tvalid_1's multi_logloss: 0.570746\n",
      "[3000]\ttraining's multi_logloss: 0.366373\tvalid_1's multi_logloss: 0.525205\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.366373\tvalid_1's multi_logloss: 0.525205\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================7============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.593635\tvalid_1's multi_logloss: 0.668308\n",
      "[2000]\ttraining's multi_logloss: 0.451859\tvalid_1's multi_logloss: 0.580179\n",
      "[3000]\ttraining's multi_logloss: 0.363631\tvalid_1's multi_logloss: 0.530823\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.363631\tvalid_1's multi_logloss: 0.530823\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================8============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.590057\tvalid_1's multi_logloss: 0.679848\n",
      "[2000]\ttraining's multi_logloss: 0.447561\tvalid_1's multi_logloss: 0.582745\n",
      "[3000]\ttraining's multi_logloss: 0.36227\tvalid_1's multi_logloss: 0.533841\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.36227\tvalid_1's multi_logloss: 0.533841\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================9============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.592992\tvalid_1's multi_logloss: 0.661719\n",
      "[2000]\ttraining's multi_logloss: 0.452537\tvalid_1's multi_logloss: 0.570209\n",
      "[3000]\ttraining's multi_logloss: 0.363009\tvalid_1's multi_logloss: 0.517196\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.363009\tvalid_1's multi_logloss: 0.517196\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================10============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[1000]\ttraining's multi_logloss: 0.592597\tvalid_1's multi_logloss: 0.669035\n",
      "[2000]\ttraining's multi_logloss: 0.44757\tvalid_1's multi_logloss: 0.574087\n",
      "[3000]\ttraining's multi_logloss: 0.362737\tvalid_1's multi_logloss: 0.52794\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's multi_logloss: 0.362737\tvalid_1's multi_logloss: 0.52794\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "sub = np.zeros((dum_test_x.shape[0], 3))\n",
    "\n",
    "for n_folds, (train_index, val_index) in enumerate(folds.split(X_train_over, y_train_over)):\n",
    "    \n",
    "    print(f'===================================={n_folds+1}============================================')\n",
    "\n",
    "    X_train, X_val = X_train_over.iloc[train_index], X_train_over.iloc[val_index]\n",
    "    y_train, y_val = y_train_over.iloc[train_index], y_train_over.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    lgb = LGBMClassifier(n_estimators=3000,\n",
    "                        is_unbalance = True, n_jobs = -1,\n",
    "                        num_leaves = 128,\n",
    "                        max_depth = 16,\n",
    "                        random_state = 507,\n",
    "                        learning_rate = 0.005)\n",
    "    lgb.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "            early_stopping_rounds=30,\n",
    "           verbose=1000)\n",
    "    y_val_onehot = pd.get_dummies(y_val)\n",
    "    prediction = lgb.predict_proba(X_val)\n",
    "    loss.append(log_loss(y_val_onehot, prediction))\n",
    "    sub+= lgb.predict_proba(dum_test_x)\n",
    "    \n",
    "    \n",
    "    print(f'================================================================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.526040539861962"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = sub/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/smote2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
